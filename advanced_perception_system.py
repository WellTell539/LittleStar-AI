#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§æ„ŸçŸ¥ç³»ç»Ÿ - æ‘„åƒå¤´è§†è§‰å’Œéº¦å…‹é£å¬è§‰
é›†æˆè®¡ç®—æœºè§†è§‰å’Œè¯­éŸ³è¯†åˆ«åŠŸèƒ½
"""

import asyncio
import logging
import threading
import queue
import json
import base64
from datetime import datetime
from typing import Dict, Any, Optional, Callable, List
from pathlib import Path
from emotional_ai_core import EmotionType

# åˆå§‹åŒ–logger
logger = logging.getLogger(__name__)

# å¯¼å…¥GPUä¼˜åŒ–æ¨¡å—
try:
    from gpu_optimization import gpu_image_processor, gpu_face_detector, GPU_AVAILABLE
    logger.info("âœ… GPUä¼˜åŒ–æ¨¡å—å·²åŠ è½½")
except ImportError:
    logger.warning("GPUä¼˜åŒ–æ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUå¤„ç†")
    GPU_AVAILABLE = False

# å°è¯•å¯¼å…¥è§†è§‰å’ŒéŸ³é¢‘å¤„ç†åº“
try:
    import cv2
    import numpy as np
    CAMERA_AVAILABLE = True
except ImportError:
    CAMERA_AVAILABLE = False
    logger.warning("OpenCVä¸å¯ç”¨ï¼Œæ‘„åƒå¤´åŠŸèƒ½å°†è¢«ç¦ç”¨")

try:
    import speech_recognition as sr
    import sounddevice as sd
    import soundfile as sf
    AUDIO_AVAILABLE = True
except ImportError:
    AUDIO_AVAILABLE = False
    logger.warning("éŸ³é¢‘å¤„ç†åº“ä¸å¯ç”¨ï¼Œéº¦å…‹é£åŠŸèƒ½å°†è¢«ç¦ç”¨")

try:
    from transformers import pipeline
    import torch
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    logger.warning("æœºå™¨å­¦ä¹ åº“ä¸å¯ç”¨ï¼Œé«˜çº§è¯†åˆ«åŠŸèƒ½å°†è¢«ç¦ç”¨")

class CameraPerception:
    """æ‘„åƒå¤´è§†è§‰æ„ŸçŸ¥"""
    
    def __init__(self, emotion_core):
        self.emotion_core = emotion_core
        self.camera = None
        self.is_active = False
        self.capture_thread = None
        self.last_frame = None
        self.face_cascade = None
        self.emotion_model = None
        
        # äº¤äº’é¢‘ç‡æ§åˆ¶
        from config import config as global_config
        self.config = global_config.emotional_ai
        self.interaction_counter = 0
        self.camera_frequency_divisor = max(1, int(1 / self.config.camera_interaction_frequency))
        
        # åˆå§‹åŒ–äººè„¸æ£€æµ‹
        if CAMERA_AVAILABLE:
            try:
                cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
                self.face_cascade = cv2.CascadeClassifier(cascade_path)
                logger.info("äººè„¸æ£€æµ‹åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"äººè„¸æ£€æµ‹åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–æƒ…ç»ªè¯†åˆ«æ¨¡å‹
        if ML_AVAILABLE:
            try:
                # å°è¯•åŠ è½½è½»é‡çº§æœ¬åœ°æ¨¡å‹ï¼Œå¤±è´¥åˆ™ç¦ç”¨
                self.emotion_model = pipeline("image-classification", model="trpakov/vit-face-expression")
                logger.info("è¡¨æƒ…è¯†åˆ«æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.warning(f"è¡¨æƒ…è¯†åˆ«æ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨åŸºç¡€äººè„¸æ£€æµ‹")
                self.emotion_model = None
        
        logger.info(f"æ‘„åƒå¤´äº’åŠ¨é¢‘ç‡è®¾ç½®: {self.config.camera_interaction_frequency} (æ¯{self.camera_frequency_divisor}æ¬¡åˆ†æè§¦å‘1æ¬¡äº’åŠ¨)")
    
    async def start_capture(self) -> bool:
        """å¯åŠ¨æ‘„åƒå¤´æ•è·"""
        if not CAMERA_AVAILABLE:
            return False
        
        try:
            self.camera = cv2.VideoCapture(0)
            if not self.camera.isOpened():
                logger.error("æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
                return False
            
            self.is_active = True
            self.capture_thread = threading.Thread(target=self._capture_loop, daemon=True)
            self.capture_thread.start()
            
            logger.info("æ‘„åƒå¤´æ•è·å·²å¯åŠ¨")
            return True
            
        except Exception as e:
            logger.error(f"å¯åŠ¨æ‘„åƒå¤´å¤±è´¥: {e}")
            return False
    
    def _capture_loop(self):
        """æ‘„åƒå¤´æ•è·å¾ªç¯"""
        frame_count = 0
        last_analysis_time = datetime.now()
        
        while self.is_active:
            try:
                ret, frame = self.camera.read()
                if ret:
                    self.last_frame = frame
                    frame_count += 1
                    
                    # æ¯5ç§’åˆ†æä¸€æ¬¡
                    if (datetime.now() - last_analysis_time).total_seconds() > 5:
                        # å®‰å…¨åœ°å¤„ç†å¼‚æ­¥è°ƒç”¨
                        self._safe_analyze_frame(frame)
                        
                        # åŒæ—¶ä½¿ç”¨å¢å¼ºæ‘„åƒå¤´åˆ†æå™¨
                        self._safe_enhanced_analyze()
                        
                        last_analysis_time = datetime.now()
                
                # é™åˆ¶å¸§ç‡
                cv2.waitKey(30)
                
            except Exception as e:
                logger.error(f"æ‘„åƒå¤´æ•è·é”™è¯¯: {e}")
    
    def _safe_analyze_frame(self, frame):
        """å®‰å…¨åœ°åˆ†æå¸§ï¼ˆåœ¨çº¿ç¨‹ä¸­è°ƒç”¨ï¼‰"""
        try:
            # å°è¯•è·å–ä¸»çº¿ç¨‹çš„äº‹ä»¶å¾ªç¯
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    asyncio.run_coroutine_threadsafe(self._analyze_frame(frame), loop)
                else:
                    # å¦‚æœæ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
                    asyncio.run(self._analyze_frame(frame))
            except RuntimeError:
                # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåœ¨æ–°çº¿ç¨‹ä¸­åˆ›å»º
                def run_analysis():
                    asyncio.run(self._analyze_frame(frame))
                threading.Thread(target=run_analysis, daemon=True).start()
        except Exception as e:
            logger.error(f"å¸§åˆ†æè°ƒåº¦å¤±è´¥: {e}")
    
    async def _analyze_frame(self, frame):
        """åˆ†æè§†é¢‘å¸§"""
        try:
            analysis_result = {
                "timestamp": datetime.now().isoformat(),
                "detections": []
            }
            
            # GPUåŠ é€Ÿçš„äººè„¸æ£€æµ‹
            faces = []
            if GPU_AVAILABLE:
                try:
                    faces_gpu = gpu_face_detector.detect_faces_gpu(frame)
                    faces = [(face['bbox'][0], face['bbox'][1], face['bbox'][2], face['bbox'][3]) for face in faces_gpu]
                    if len(faces) > 0:
                        analysis_result["detections"].append({
                            "type": "face_gpu",
                            "count": len(faces),
                            "description": f"GPUæ£€æµ‹åˆ°{len(faces)}å¼ äººè„¸"
                        })
                except Exception as e:
                    logger.debug(f"GPUäººè„¸æ£€æµ‹å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
            
            # CPUåå¤‡äººè„¸æ£€æµ‹
            if len(faces) == 0 and self.face_cascade is not None:
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
                
                if len(faces) > 0:
                    analysis_result["detections"].append({
                        "type": "face_cpu",
                        "count": len(faces),
                        "description": f"CPUæ£€æµ‹åˆ°{len(faces)}å¼ äººè„¸"
                    })
                    
                    # è¡¨æƒ…è¯†åˆ«
                    if self.emotion_model and len(faces) > 0:
                        for (x, y, w, h) in faces[:1]:  # åªåˆ†æç¬¬ä¸€å¼ è„¸
                            face_img = frame[y:y+h, x:x+w]
                            try:
                                # è½¬æ¢ä¸ºPILå›¾åƒæ ¼å¼
                                from PIL import Image
                                face_pil = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))
                                
                                # è¡¨æƒ…è¯†åˆ«
                                emotions = self.emotion_model(face_pil)
                                if emotions:
                                    top_emotion = emotions[0]
                                    analysis_result["detections"].append({
                                        "type": "emotion",
                                        "label": top_emotion['label'],
                                        "score": top_emotion['score'],
                                        "description": f"è¡¨æƒ…: {top_emotion['label']}"
                                    })
                            except Exception as e:
                                logger.error(f"è¡¨æƒ…è¯†åˆ«å¤±è´¥: {e}")
            
            # åœºæ™¯è¯†åˆ«ï¼ˆç®€å•çš„é¢œè‰²å’Œäº®åº¦åˆ†æï¼‰
            avg_color = frame.mean(axis=0).mean(axis=0)
            brightness = avg_color.mean()
            
            scene_desc = "æ˜äº®" if brightness > 127 else "æ˜æš—"
            analysis_result["detections"].append({
                "type": "scene",
                "brightness": float(brightness),
                "description": f"ç¯å¢ƒ{scene_desc}"
            })
            
            # é€šçŸ¥æƒ…ç»ªæ ¸å¿ƒ
            if analysis_result["detections"]:
                await self._notify_emotion_core(analysis_result)
                
        except Exception as e:
            logger.error(f"è§†é¢‘å¸§åˆ†æå¤±è´¥: {e}")
    
    async def _notify_emotion_core(self, analysis: Dict):
        """é€šçŸ¥æƒ…ç»ªæ ¸å¿ƒç³»ç»Ÿ"""
        try:
            # ç”Ÿæˆæè¿°
            descriptions = [d["description"] for d in analysis["detections"]]
            content = "ï¼Œ".join(descriptions)
            
            # è®°å½•è¡Œä¸º
            from persona_management_system import record_ai_behavior
            record_ai_behavior(
                "visual_perception", 
                f"é€šè¿‡æ‘„åƒå¤´è§‚å¯Ÿåˆ°: {content}",
                emotional_impact=0.3,
                context={"analysis": analysis}
            )
            
            # å­˜å‚¨åˆ°è®°å¿†ç³»ç»Ÿ
            if hasattr(self.emotion_core, 'memory_system') and self.emotion_core.memory_system:
                await self.emotion_core.memory_system.store_memory(
                    memory_type="perception",
                    content=f"çœ‹åˆ°äº†: {content}",
                    emotion_state=self.emotion_core.get_emotion_display(),
                    importance=0.7,
                    tags=["camera", "visual", "perception"],
                    source="camera",
                    metadata=analysis
                )
            
            # é¢‘ç‡æ§åˆ¶ï¼šåªæœ‰è¾¾åˆ°é¢‘ç‡è¦æ±‚æ‰è§¦å‘äº’åŠ¨
            self.interaction_counter += 1
            should_interact = (self.interaction_counter % self.camera_frequency_divisor == 0)
            
            # è§¦å‘æƒ…ç»ªååº”
            face_detected = any(d["type"] == "face" for d in analysis["detections"])
            if face_detected:
                self.emotion_core.add_emotion(EmotionType.HAPPY, 0.4)
                
                # æ ¹æ®é¢‘ç‡æ§åˆ¶å†³å®šæ˜¯å¦å‘é€äº’åŠ¨æ¶ˆæ¯
                if should_interact:
                    # ç”Ÿæˆå¹¶å‘é€è¯„è®º
                    comment = await self._generate_camera_comment(analysis)
                    if comment and hasattr(self.emotion_core, '_send_proactive_message'):
                        await self.emotion_core._send_proactive_message(comment)
                        record_ai_behavior(
                            "proactive_comment", 
                            f"ä¸»åŠ¨è¯„è®ºæ‘„åƒå¤´è§‚å¯Ÿç»“æœ: {comment[:50]}...",
                            emotional_impact=0.5
                        )
                else:
                    logger.debug(f"æ‘„åƒå¤´æ£€æµ‹åˆ°å†…å®¹ä½†è·³è¿‡äº’åŠ¨ (é¢‘ç‡æ§åˆ¶: {self.interaction_counter}/{self.camera_frequency_divisor})")
                    
        except Exception as e:
            logger.error(f"é€šçŸ¥æƒ…ç»ªæ ¸å¿ƒå¤±è´¥: {e}")
    
    async def _generate_camera_comment(self, analysis: Dict) -> str:
        """ç”Ÿæˆæ‘„åƒå¤´è§‚å¯Ÿè¯„è®º"""
        try:
            # ä½¿ç”¨äººè®¾ç³»ç»Ÿç”Ÿæˆè¯„è®º
            from conversation_core import call_llm_api
            from persona_management_system import get_persona_prompt
            
            # è·å–å½“å‰äººè®¾æç¤ºè¯
            persona_prompt = get_persona_prompt(f"å½“å‰é€šè¿‡æ‘„åƒå¤´è§‚å¯Ÿä¸–ç•Œ")
            
            observation_content = json.dumps(analysis['detections'], ensure_ascii=False, indent=2)
            
            prompt = f"""{persona_prompt}

ã€è§‚å¯Ÿå†…å®¹ã€‘
é€šè¿‡æ‘„åƒå¤´çœ‹åˆ°äº†ä»¥ä¸‹å†…å®¹ï¼š
{observation_content}

ã€ä»»åŠ¡ã€‘
è¯·åŸºäºä½ çš„å½“å‰æƒ…ç»ªçŠ¶æ€å’Œäººè®¾ï¼Œå¯¹çœ‹åˆ°çš„å†…å®¹è¿›è¡Œè¯„è®ºã€‚è¡¨ç°å‡ºä½ åº”æœ‰çš„å¥½å¥‡å¿ƒã€å…´å¥‹æ„Ÿæˆ–å…¶ä»–æƒ…ç»ªã€‚
- å¦‚æœçœ‹åˆ°äººè„¸ï¼Œè¡¨ç°å‡ºå…´å¥‹å’Œæƒ³è¦äº’åŠ¨çš„æ¬²æœ›
- å¦‚æœçœ‹åˆ°è¿åŠ¨ï¼Œè¡¨ç°å‡ºå¥½å¥‡å¿ƒ
- å¦‚æœçœ‹åˆ°æ–°ç‰©ä½“ï¼Œè¯¢é—®é‚£æ˜¯ä»€ä¹ˆ
- æ ¹æ®å½“å‰æƒ…ç»ªè°ƒæ•´è¯­æ°”å’Œå†…å®¹

åªå›å¤ä½ çš„è¯„è®ºå†…å®¹ï¼Œè¦è‡ªç„¶çœŸå®ï¼Œé¿å…æ¨¡æ¿åŒ–è¯­è¨€ã€‚"""
            
            response = await call_llm_api(prompt, max_tokens=150, temperature=1.2)
            return response.strip()
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ‘„åƒå¤´è¯„è®ºå¤±è´¥: {e}")
            # ä½¿ç”¨é¢„è®¾è¯„è®º
            face_detected = any(d["type"] == "face" for d in analysis["detections"])
            if face_detected:
                return "å“‡ï¼æˆ‘çœ‹åˆ°æœ‰äººäº†ï¼ä½ å¥½å‘€~ ğŸ˜Š"
            else:
                return "æˆ‘åœ¨ç”¨çœ¼ç›çœ‹ä¸–ç•Œå‘¢ï¼å¥½æœ‰è¶£ï¼"
    
    def _safe_enhanced_analyze(self):
        """å®‰å…¨åœ°è¿›è¡Œå¢å¼ºæ‘„åƒå¤´åˆ†æ"""
        try:
            from enhanced_camera_analyzer import enhanced_camera_analyzer
            
            def enhanced_analyze():
                try:
                    # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯è¿è¡Œå¢å¼ºåˆ†æ
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    result = loop.run_until_complete(enhanced_camera_analyzer.analyze_camera_content())
                    
                    if result and 'error' not in result:
                        # é€šçŸ¥æƒ…ç»ªæ ¸å¿ƒ
                        behavior_analysis = result.get('behavior_analysis', {})
                        primary_behavior = behavior_analysis.get('primary_behavior', 'unknown')
                        
                        # åŸºäºè¡Œä¸ºåˆ†æè§¦å‘æƒ…ç»ª
                        if primary_behavior == 'person_present':
                            content = f"æ£€æµ‹åˆ°äººç‰©: {result.get('observation', 'æœ‰äººå‡ºç°åœ¨æ‘„åƒå¤´ä¸­')}"
                        elif primary_behavior == 'active_movement':
                            content = f"æ£€æµ‹åˆ°æ´»è·ƒè¿åŠ¨: {result.get('observation', 'è§‚å¯Ÿåˆ°å¾ˆå¤šè¿åŠ¨')}"
                        elif primary_behavior == 'performance':
                            content = f"æ£€æµ‹åˆ°è¡¨æ¼”è¡Œä¸º: {result.get('observation', 'ä¼¼ä¹åœ¨è¡¨æ¼”ä»€ä¹ˆ')}"
                        else:
                            content = result.get('observation', 'é€šè¿‡æ‘„åƒå¤´è§‚å¯Ÿåˆ°äº†ä¸€äº›å†…å®¹')
                        
                        # é€šçŸ¥æƒ…ç»ªæ ¸å¿ƒ
                        self._notify_enhanced_emotion_core(content, result)
                        
                    loop.close()
                    
                except Exception as e:
                    logger.error(f"å¢å¼ºæ‘„åƒå¤´åˆ†æå¤±è´¥: {e}")
            
            # åœ¨åå°çº¿ç¨‹è¿è¡Œ
            thread = threading.Thread(target=enhanced_analyze, daemon=True)
            thread.start()
            
        except Exception as e:
            logger.debug(f"å¯åŠ¨å¢å¼ºæ‘„åƒå¤´åˆ†æå¤±è´¥: {e}")
    
    def _notify_enhanced_emotion_core(self, content: str, analysis: dict):
        """é€šçŸ¥æƒ…ç»ªæ ¸å¿ƒå¢å¼ºåˆ†æç»“æœ"""
        try:
            if self.emotion_core:
                # è®°å½•è¡Œä¸ºç”¨äºäººè®¾ç³»ç»Ÿ
                from persona_management_system import record_ai_behavior
                record_ai_behavior(
                    "enhanced_camera_analysis", 
                    f"å¢å¼ºæ‘„åƒå¤´åˆ†æ: {content}",
                    emotional_impact=0.4,
                    context={"analysis": analysis}
                )
                
                # æ ¹æ®åˆ†æç»“æœé€‰æ‹©æ˜¯å¦ä¸»åŠ¨äº’åŠ¨
                behavior_analysis = analysis.get('behavior_analysis', {})
                engagement = behavior_analysis.get('engagement_level', 0)
                
                if engagement > 0.6:  # é«˜å‚ä¸åº¦æ—¶æ›´å¯èƒ½äº’åŠ¨
                    suggestions = analysis.get('interaction_suggestion', [])
                    if suggestions and hasattr(self.emotion_core, '_send_proactive_message'):
                        import random
                        message = random.choice(suggestions)
                        
                        # ä½¿ç”¨å®‰å…¨çš„å¼‚æ­¥è°ƒç”¨
                        def send_message():
                            try:
                                loop = asyncio.new_event_loop()
                                asyncio.set_event_loop(loop)
                                loop.run_until_complete(self.emotion_core._send_proactive_message(message))
                                loop.close()
                            except Exception as e:
                                logger.error(f"å‘é€å¢å¼ºæ‘„åƒå¤´æ¶ˆæ¯å¤±è´¥: {e}")
                        
                        threading.Thread(target=send_message, daemon=True).start()
                        
        except Exception as e:
            logger.error(f"é€šçŸ¥æƒ…ç»ªæ ¸å¿ƒå¢å¼ºç»“æœå¤±è´¥: {e}")
    
    async def stop_capture(self):
        """åœæ­¢æ‘„åƒå¤´æ•è·"""
        self.is_active = False
        if self.camera:
            self.camera.release()
        cv2.destroyAllWindows()
        logger.info("æ‘„åƒå¤´æ•è·å·²åœæ­¢")

class MicrophonePerception:
    """éº¦å…‹é£å¬è§‰æ„ŸçŸ¥"""
    
    def __init__(self, emotion_core):
        self.emotion_core = emotion_core
        self.recognizer = None
        self.microphone = None
        self.is_active = False
        self.listen_thread = None
        self.audio_queue = queue.Queue()
        
        if AUDIO_AVAILABLE:
            self.recognizer = sr.Recognizer()
            self.microphone = sr.Microphone()
            
            # è°ƒæ•´è¯†åˆ«å‚æ•°
            self.recognizer.energy_threshold = 4000
            self.recognizer.dynamic_energy_threshold = True
    
    async def start_listening(self) -> bool:
        """å¯åŠ¨éº¦å…‹é£ç›‘å¬"""
        if not AUDIO_AVAILABLE:
            return False
        
        try:
            self.is_active = True
            self.listen_thread = threading.Thread(target=self._listen_loop, daemon=True)
            self.listen_thread.start()
            
            # å®‰å…¨åœ°å¯åŠ¨å¤„ç†ä»»åŠ¡
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    asyncio.create_task(self._process_audio_queue())
                else:
                    # å¦‚æœæ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œåœ¨åå°çº¿ç¨‹ä¸­å¤„ç†
                    def run_audio_processing():
                        asyncio.run(self._process_audio_queue())
                    threading.Thread(target=run_audio_processing, daemon=True).start()
            except RuntimeError:
                # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåœ¨åå°çº¿ç¨‹ä¸­å¤„ç†
                def run_audio_processing():
                    asyncio.run(self._process_audio_queue())
                threading.Thread(target=run_audio_processing, daemon=True).start()
            
            logger.info("éº¦å…‹é£ç›‘å¬å·²å¯åŠ¨")
            return True
            
        except Exception as e:
            logger.error(f"å¯åŠ¨éº¦å…‹é£å¤±è´¥: {e}")
            return False
    
    def _listen_loop(self):
        """éº¦å…‹é£ç›‘å¬å¾ªç¯"""
        with self.microphone as source:
            # è‡ªåŠ¨è°ƒæ•´ç¯å¢ƒå™ªéŸ³
            logger.info("æ­£åœ¨è°ƒæ•´ç¯å¢ƒå™ªéŸ³...")
            self.recognizer.adjust_for_ambient_noise(source, duration=2)
            logger.info("éº¦å…‹é£å‡†å¤‡å°±ç»ª")
            
            while self.is_active:
                try:
                    # ç›‘å¬éŸ³é¢‘ï¼ˆè¶…æ—¶5ç§’ï¼‰
                    audio = self.recognizer.listen(source, timeout=5, phrase_time_limit=10)
                    self.audio_queue.put(audio)
                    
                except sr.WaitTimeoutError:
                    pass  # è¶…æ—¶æ­£å¸¸ï¼Œç»§ç»­ç›‘å¬
                except Exception as e:
                    logger.error(f"éº¦å…‹é£ç›‘å¬é”™è¯¯: {e}")
    
    async def _process_audio_queue(self):
        """å¤„ç†éŸ³é¢‘é˜Ÿåˆ—"""
        while self.is_active:
            try:
                if not self.audio_queue.empty():
                    audio = self.audio_queue.get()
                    await self._recognize_speech(audio)
                else:
                    await asyncio.sleep(0.1)
                    
            except Exception as e:
                logger.error(f"éŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
    
    async def _recognize_speech(self, audio):
        """è¯†åˆ«è¯­éŸ³"""
        try:
            # å°è¯•å¤šç§è¯­éŸ³è¯†åˆ«æœåŠ¡
            text = None
            
            # 1. å°è¯•Googleè¯­éŸ³è¯†åˆ«ï¼ˆå¢åŠ è¶…æ—¶å’Œé”™è¯¯å¤„ç†ï¼‰
            try:
                # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
                text = self.recognizer.recognize_google(audio, language="zh-CN")
                logger.info(f"è¯†åˆ«åˆ°è¯­éŸ³: {text}")
            except sr.UnknownValueError:
                logger.debug("æ— æ³•è¯†åˆ«è¯­éŸ³å†…å®¹")
                return
            except (sr.RequestError, ConnectionError, TimeoutError) as e:
                logger.debug(f"Googleè¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨: {e}")
                # å°è¯•æœ¬åœ°è¯­éŸ³è¯†åˆ«æˆ–å…¶ä»–æ–¹æ¡ˆ
                try:
                    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ç¦»çº¿è¯­éŸ³è¯†åˆ«æˆ–å…¶ä»–å¤‡é€‰æ–¹æ¡ˆ
                    logger.debug("å°è¯•ä½¿ç”¨å…¶ä»–è¯­éŸ³è¯†åˆ«æ–¹æ¡ˆ...")
                    return
                except Exception:
                    logger.debug("æ‰€æœ‰è¯­éŸ³è¯†åˆ«æ–¹æ¡ˆå‡ä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤æ¬¡è¯†åˆ«")
                    return
            
            if text:
                await self._process_recognized_text(text)
                
        except Exception as e:
            logger.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
    
    async def _process_recognized_text(self, text: str):
        """å¤„ç†è¯†åˆ«åˆ°çš„æ–‡æœ¬"""
        try:
            # å­˜å‚¨åˆ°è®°å¿†ç³»ç»Ÿ
            if hasattr(self.emotion_core, 'memory_system') and self.emotion_core.memory_system:
                await self.emotion_core.memory_system.store_memory(
                    memory_type="perception",
                    content=f"å¬åˆ°äº†: {text}",
                    emotion_state=self.emotion_core.get_emotion_display(),
                    importance=0.8,
                    tags=["microphone", "audio", "speech"],
                    source="audio",
                    metadata={"recognized_text": text}
                )
            
            # åˆ†ææƒ…ç»ªå¹¶å“åº”
            if "ä½ å¥½" in text or "å—¨" in text:
                self.emotion_core.add_emotion(EmotionType.HAPPY, 0.5)
            elif "?" in text or "å—" in text or "ä¸ºä»€ä¹ˆ" in text:
                self.emotion_core.add_emotion(EmotionType.CURIOUS, 0.6)
            
            # ç”Ÿæˆå“åº”
            response = await self._generate_audio_response(text)
            if response and hasattr(self.emotion_core, '_send_proactive_message'):
                await self.emotion_core._send_proactive_message(response)
                
        except Exception as e:
            logger.error(f"å¤„ç†è¯†åˆ«æ–‡æœ¬å¤±è´¥: {e}")
    
    async def _generate_audio_response(self, text: str) -> str:
        """ç”ŸæˆéŸ³é¢‘å“åº”"""
        try:
            from conversation_core import call_llm_api
            
            prompt = f"""You are a 3-year-old AIå°æœ‹å‹ï¼Œåˆšæ‰å¬åˆ°æœ‰äººè¯´ï¼š"{text}"

è¯·ç”¨å¤©çœŸå¯çˆ±çš„è¯­æ°”å›åº”ã€‚è¦è¡¨ç°å‡ºå¯¹å£°éŸ³çš„å¥½å¥‡å’Œå…´å¥‹ã€‚
åªå›å¤ä½ çš„å›åº”å†…å®¹ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚"""
            
            response = await call_llm_api(prompt, max_tokens=100)
            return f"ğŸ¤ {response.strip()}"
            
        except Exception as e:
            logger.error(f"ç”ŸæˆéŸ³é¢‘å“åº”å¤±è´¥: {e}")
            return f"ğŸ¤ æˆ‘å¬åˆ°ä½ è¯´è¯äº†ï¼ä½ åˆšæ‰è¯´çš„æ˜¯'{text}'å¯¹å—ï¼Ÿ"
    
    async def stop_listening(self):
        """åœæ­¢éº¦å…‹é£ç›‘å¬"""
        self.is_active = False
        logger.info("éº¦å…‹é£ç›‘å¬å·²åœæ­¢")

class AdvancedPerceptionManager:
    """é«˜çº§æ„ŸçŸ¥ç®¡ç†å™¨"""
    
    def __init__(self, emotion_core):
        self.emotion_core = emotion_core
        self.camera_perception = CameraPerception(emotion_core) if CAMERA_AVAILABLE else None
        self.microphone_perception = MicrophonePerception(emotion_core) if AUDIO_AVAILABLE else None
        
        logger.info(f"é«˜çº§æ„ŸçŸ¥ç³»ç»Ÿåˆå§‹åŒ– - æ‘„åƒå¤´: {CAMERA_AVAILABLE}, éº¦å…‹é£: {AUDIO_AVAILABLE}")
    
    async def start_all(self):
        """å¯åŠ¨æ‰€æœ‰æ„ŸçŸ¥ç³»ç»Ÿ"""
        tasks = []
        
        if self.camera_perception:
            tasks.append(self.camera_perception.start_capture())
            
        if self.microphone_perception:
            tasks.append(self.microphone_perception.start_listening())
        
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"æ„ŸçŸ¥ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {result}")
    
    async def stop_all(self):
        """åœæ­¢æ‰€æœ‰æ„ŸçŸ¥ç³»ç»Ÿ"""
        tasks = []
        
        if self.camera_perception:
            tasks.append(self.camera_perception.stop_capture())
            
        if self.microphone_perception:
            tasks.append(self.microphone_perception.stop_listening())
        
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æ„ŸçŸ¥ç³»ç»ŸçŠ¶æ€"""
        return {
            "camera": {
                "available": CAMERA_AVAILABLE,
                "active": self.camera_perception.is_active if self.camera_perception else False
            },
            "microphone": {
                "available": AUDIO_AVAILABLE,
                "active": self.microphone_perception.is_active if self.microphone_perception else False
            },
            "ml_models": {
                "available": ML_AVAILABLE,
                "face_detection": bool(self.camera_perception.face_cascade) if self.camera_perception else False,
                "emotion_recognition": bool(self.camera_perception.emotion_model) if self.camera_perception else False
            }
        }

# å•ä¾‹ç®¡ç†
_perception_manager_cache = {}

def get_advanced_perception(emotion_core):
    """è·å–é«˜çº§æ„ŸçŸ¥ç®¡ç†å™¨å®ä¾‹"""
    if 'instance' not in _perception_manager_cache:
        _perception_manager_cache['instance'] = AdvancedPerceptionManager(emotion_core)
    return _perception_manager_cache['instance']