#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPUæ¨ç†é…ç½®æ¨¡å— - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ¨¡å‹çš„GPUé…ç½®
"""

import logging
import torch
import os
from typing import Optional, Dict, Any, Union
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class GPUConfig:
    """GPUé…ç½®ç±»"""
    device: str = "auto"  # "auto", "cpu", "cuda", "mps"
    mixed_precision: bool = True  # æ··åˆç²¾åº¦è®­ç»ƒ
    memory_fraction: float = 0.8  # GPUæ˜¾å­˜ä½¿ç”¨æ¯”ä¾‹
    cache_dir: str = "./models_cache"  # æ¨¡å‹ç¼“å­˜ç›®å½•
    batch_size: int = 8  # æ‰¹å¤„ç†å¤§å°
    max_length: int = 512  # æœ€å¤§åºåˆ—é•¿åº¦

class GPUInferenceManager:
    """GPUæ¨ç†ç®¡ç†å™¨"""
    
    def __init__(self, config: Optional[GPUConfig] = None):
        self.config = config or GPUConfig()
        self.device = self._detect_device()
        self.models_cache = {}
        
        # è®¾ç½®GPUå†…å­˜ç®¡ç†
        self._setup_gpu_memory()
        
        logger.info(f"ğŸ¯ GPUæ¨ç†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}")
    
    def _detect_device(self) -> str:
        """è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾å¤‡"""
        if self.config.device != "auto":
            return self.config.device
        
        # æ£€æŸ¥CUDA
        if torch.cuda.is_available():
            device = f"cuda:{torch.cuda.current_device()}"
            logger.info(f"âœ… æ£€æµ‹åˆ°CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
            return device
        
        # æ£€æŸ¥Apple Silicon MPS
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            logger.info("âœ… æ£€æµ‹åˆ°Apple MPSè®¾å¤‡")
            return "mps"
        
        # å›é€€åˆ°CPU
        logger.info("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")
        return "cpu"
    
    def _setup_gpu_memory(self):
        """è®¾ç½®GPUå†…å­˜ç®¡ç†"""
        if "cuda" in self.device:
            try:
                # è®¾ç½®æ˜¾å­˜åˆ†é…ç­–ç•¥
                torch.cuda.set_per_process_memory_fraction(self.config.memory_fraction)
                torch.cuda.empty_cache()
                
                # å¯ç”¨cudnnåŸºå‡†æµ‹è¯•ä»¥ä¼˜åŒ–æ€§èƒ½
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                
                logger.info(f"ğŸ“Š GPUæ˜¾å­˜é…ç½®: {self.config.memory_fraction*100}%")
                
            except Exception as e:
                logger.warning(f"GPUå†…å­˜é…ç½®å¤±è´¥: {e}")
    
    def get_model_device_map(self, model_name: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹è®¾å¤‡æ˜ å°„é…ç½®"""
        if "cuda" not in self.device:
            return {"": self.device}
        
        # æ£€æŸ¥å¯ç”¨GPUæ•°é‡
        gpu_count = torch.cuda.device_count()
        if gpu_count > 1:
            # å¤šGPUé…ç½®
            return {
                "transformer.word_embeddings": 0,
                "transformer.word_embeddings_layernorm": 0,
                "lm_head": "cpu",  # å°†è¾“å‡ºå±‚æ”¾åœ¨CPUä¸ŠèŠ‚çœæ˜¾å­˜
                "transformer.h": {i: i % gpu_count for i in range(24)}  # å‡è®¾24å±‚
            }
        else:
            # å•GPUé…ç½®
            return {"": 0}
    
    def get_model_kwargs(self, model_name: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹åŠ è½½çš„å…³é”®å­—å‚æ•°"""
        kwargs = {
            "cache_dir": self.config.cache_dir,
            "torch_dtype": torch.float16 if "cuda" in self.device else torch.float32,
            "device_map": self.get_model_device_map(model_name),
            "trust_remote_code": True,
        }
        
        # GPUç‰¹å®šé…ç½®
        if "cuda" in self.device:
            kwargs.update({
                "load_in_8bit": False,  # å¯é€‰ï¼š8ä½é‡åŒ–
                "load_in_4bit": False,  # å¯é€‰ï¼š4ä½é‡åŒ–
                "low_cpu_mem_usage": True,
                "use_cache": True,
            })
        
        return kwargs
    
    def optimize_model(self, model, model_name: str = "unknown"):
        """ä¼˜åŒ–æ¨¡å‹ä»¥æé«˜æ¨ç†é€Ÿåº¦"""
        try:
            # ç§»åŠ¨æ¨¡å‹åˆ°æ­£ç¡®è®¾å¤‡
            if hasattr(model, 'to'):
                model = model.to(self.device)
            
            # è®¾ç½®è¯„ä¼°æ¨¡å¼
            if hasattr(model, 'eval'):
                model.eval()
            
            # ç¦ç”¨æ¢¯åº¦è®¡ç®—
            if hasattr(model, 'requires_grad_'):
                for param in model.parameters():
                    param.requires_grad_(False)
            
            # ç¼–è¯‘æ¨¡å‹ï¼ˆPyTorch 2.0+ï¼‰
            if hasattr(torch, 'compile') and "cuda" in self.device:
                try:
                    model = torch.compile(model)
                    logger.info(f"âœ… æ¨¡å‹ {model_name} å·²ç¼–è¯‘ä¼˜åŒ–")
                except Exception as e:
                    logger.debug(f"æ¨¡å‹ç¼–è¯‘å¤±è´¥ï¼ˆæ­£å¸¸ï¼Œç»§ç»­ä½¿ç”¨åŸæ¨¡å‹ï¼‰: {e}")
            
            logger.info(f"ğŸš€ æ¨¡å‹ {model_name} å·²ä¼˜åŒ– - è®¾å¤‡: {self.device}")
            return model
            
        except Exception as e:
            logger.error(f"æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")
            return model
    
    def create_pipeline(self, task: str, model: str, **kwargs):
        """åˆ›å»ºä¼˜åŒ–çš„æ¨ç†ç®¡é“"""
        try:
            from transformers import pipeline
            
            # åˆå¹¶é…ç½®
            pipeline_kwargs = {
                "device": 0 if "cuda" in self.device else -1,
                "torch_dtype": torch.float16 if "cuda" in self.device else torch.float32,
                **kwargs
            }
            
            # åˆ›å»ºç®¡é“
            pipe = pipeline(
                task=task,
                model=model,
                **pipeline_kwargs
            )
            
            logger.info(f"âœ… åˆ›å»º {task} ç®¡é“æˆåŠŸ - æ¨¡å‹: {model}")
            return pipe
            
        except Exception as e:
            logger.error(f"åˆ›å»ºç®¡é“å¤±è´¥: {e}")
            raise
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–GPUå†…å­˜ä½¿ç”¨ç»Ÿè®¡"""
        stats = {"device": self.device}
        
        if "cuda" in self.device:
            try:
                stats.update({
                    "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                    "reserved_gb": torch.cuda.memory_reserved() / 1024**3,
                    "max_allocated_gb": torch.cuda.max_memory_allocated() / 1024**3,
                    "device_name": torch.cuda.get_device_name(),
                    "device_count": torch.cuda.device_count(),
                })
            except Exception as e:
                stats["error"] = str(e)
        
        return stats
    
    def clear_cache(self):
        """æ¸…ç†GPUç¼“å­˜"""
        if "cuda" in self.device:
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ GPUç¼“å­˜å·²æ¸…ç†")
    
    def benchmark_model(self, model, input_data, iterations: int = 10):
        """å¯¹æ¨¡å‹è¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        import time
        
        # é¢„çƒ­
        for _ in range(3):
            with torch.no_grad():
                _ = model(input_data)
        
        # åŸºå‡†æµ‹è¯•
        torch.cuda.synchronize() if "cuda" in self.device else None
        start_time = time.time()
        
        for _ in range(iterations):
            with torch.no_grad():
                _ = model(input_data)
        
        torch.cuda.synchronize() if "cuda" in self.device else None
        end_time = time.time()
        
        avg_time = (end_time - start_time) / iterations
        logger.info(f"ğŸ“Š æ¨¡å‹å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.4f}ç§’")
        
        return avg_time

# å…¨å±€GPUæ¨ç†ç®¡ç†å™¨å®ä¾‹
gpu_manager = GPUInferenceManager()

def get_gpu_manager() -> GPUInferenceManager:
    """è·å–å…¨å±€GPUç®¡ç†å™¨å®ä¾‹"""
    return gpu_manager

def configure_gpu_inference(device: str = "auto", memory_fraction: float = 0.8):
    """é…ç½®GPUæ¨ç†è®¾ç½®"""
    global gpu_manager
    config = GPUConfig(device=device, memory_fraction=memory_fraction)
    gpu_manager = GPUInferenceManager(config)
    return gpu_manager

# ä¾¿æ·å‡½æ•°
def load_model_optimized(model_class, model_name: str, **kwargs):
    """åŠ è½½å¹¶ä¼˜åŒ–æ¨¡å‹"""
    manager = get_gpu_manager()
    
    # åˆå¹¶é…ç½®
    model_kwargs = manager.get_model_kwargs(model_name)
    model_kwargs.update(kwargs)
    
    # åŠ è½½æ¨¡å‹
    model = model_class.from_pretrained(model_name, **model_kwargs)
    
    # ä¼˜åŒ–æ¨¡å‹
    model = manager.optimize_model(model, model_name)
    
    return model

def create_inference_pipeline(task: str, model_name: str, **kwargs):
    """åˆ›å»ºæ¨ç†ç®¡é“"""
    manager = get_gpu_manager()
    return manager.create_pipeline(task, model_name, **kwargs)

# å¯¼å‡ºä¸»è¦æ¥å£
__all__ = [
    'GPUConfig',
    'GPUInferenceManager', 
    'gpu_manager',
    'get_gpu_manager',
    'configure_gpu_inference',
    'load_model_optimized',
    'create_inference_pipeline'
]