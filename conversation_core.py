import logging
import os
# import asyncio # æ—¥å¿—ä¸ç³»ç»Ÿ
from datetime import datetime # æ—¶é—´
from mcpserver.mcp_manager import get_mcp_manager # å¤šåŠŸèƒ½ç®¡ç†
from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX # handoffæç¤ºè¯
# from mcpserver.agent_playwright_master import ControllerAgent, BrowserAgent, ContentAgent # å¯¼å…¥æµè§ˆå™¨ç›¸å…³ç±»
from openai import OpenAI,AsyncOpenAI # LLM
# import difflib # æ¨¡ç³ŠåŒ¹é…
import sys
import json
import traceback
import time # æ—¶é—´æˆ³æ‰“å°
import re # æ·»åŠ reæ¨¡å—å¯¼å…¥
from typing import List, Dict # ä¿®å¤Listæœªå¯¼å…¥
# æ¢å¤æ ‘çŠ¶æ€è€ƒç³»ç»Ÿå¯¼å…¥
from thinking import TreeThinkingEngine # æ ‘çŠ¶æ€è€ƒå¼•æ“
from thinking.config import COMPLEX_KEYWORDS # å¤æ‚å…³é”®è¯
from config import config
# å¯¼å…¥ç‹¬ç«‹çš„å·¥å…·è°ƒç”¨æ¨¡å—
from apiserver.tool_call_utils import parse_tool_calls, execute_tool_calls, tool_call_loop
# å¯¼å…¥æƒ…ç»ªAIæ ¸å¿ƒ
from emotional_ai_core import get_emotion_core

# å¯¼å…¥åŠ¨æ€å‘å¸ƒå™¨
try:
    from ai_dynamic_publisher import (
        publish_thinking, 
        publish_manual_dynamic,
        ai_dynamic_publisher
    )
    DYNAMIC_PUBLISHER_AVAILABLE = True
except ImportError:
    DYNAMIC_PUBLISHER_AVAILABLE = False

# GRAGè®°å¿†ç³»ç»Ÿå¯¼å…¥
if config.grag.enabled:
    try:
        from summer_memory.memory_manager import memory_manager
    except Exception as e:
        logger = logging.getLogger("NagaConversation")
        logger.error(f"å¤å›­è®°å¿†ç³»ç»ŸåŠ è½½å¤±è´¥: {e}")
        memory_manager = None
else:
    memory_manager = None

def now():
    return time.strftime('%H:%M:%S:')+str(int(time.time()*1000)%10000) # å½“å‰æ—¶é—´
_builtin_print=print
def print(*a, **k):
    return sys.stderr.write('[print] '+(' '.join(map(str,a)))+'\n')

# é…ç½®æ—¥å¿— - ä½¿ç”¨ç»Ÿä¸€é…ç½®ç³»ç»Ÿçš„æ—¥å¿—çº§åˆ«
log_level = getattr(logging, config.system.log_level.upper(), logging.INFO)
logging.basicConfig(
    level=log_level,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stderr)
    ]
)

# ç‰¹åˆ«è®¾ç½®httpcoreå’Œopenaiçš„æ—¥å¿—çº§åˆ«ï¼Œå‡å°‘è¿æ¥å¼‚å¸¸å™ªéŸ³
logging.getLogger("httpcore.connection").setLevel(logging.WARNING)
logging.getLogger("httpcore.http11").setLevel(logging.WARNING)  # å±è”½HTTPè¯·æ±‚DEBUG
logging.getLogger("httpx").setLevel(logging.WARNING)  # å±è”½httpx DEBUG
logging.getLogger("openai._base_client").setLevel(logging.WARNING)
# éšè—asyncioçš„DEBUGæ—¥å¿—
logging.getLogger("asyncio").setLevel(logging.WARNING)
logger = logging.getLogger("NagaConversation")

# _MCP_HANDOFF_REGISTERED=False  # å·²ç§»é™¤ï¼Œä¸å†éœ€è¦
_TREE_THINKING_SUBSYSTEMS_INITIALIZED=False
_MCP_SERVICES_INITIALIZED=False
_QUICK_MODEL_MANAGER_INITIALIZED=False
_VOICE_ENABLED_LOGGED=False

class NagaConversation: # å¯¹è¯ä¸»ç±»
    def __init__(self):
        self.mcp = get_mcp_manager()
        self.messages = []
        self.dev_mode = False
        
        # æ£€æµ‹æ¼”ç¤ºæ¨¡å¼
        self.demo_mode = os.getenv("NAGAAGENT_DEMO_MODE") == "1" or config.api.api_key == "demo-mode"
        
        if self.demo_mode:
            logger.info("ğŸ­ æ¼”ç¤ºæ¨¡å¼å·²å¯ç”¨ - ä½¿ç”¨é¢„è®¾å“åº”")
            self.client = None
            self.async_client = None
            # å¯¼å…¥æ¼”ç¤ºå¯¹è¯æ¨¡å—
            try:
                from demo_conversation import get_demo_response
                self.demo_response_func = get_demo_response
            except ImportError:
                logger.warning("æ¼”ç¤ºå¯¹è¯æ¨¡å—æœªæ‰¾åˆ°ï¼Œä½¿ç”¨å†…ç½®å“åº”")
                self.demo_response_func = self._builtin_demo_response
        else:
            self.client = OpenAI(api_key=config.api.api_key, base_url=config.api.base_url.rstrip('/') + '/')
            self.async_client = AsyncOpenAI(api_key=config.api.api_key, base_url=config.api.base_url.rstrip('/') + '/')
        
        # åˆå§‹åŒ–MCPæœåŠ¡ç³»ç»Ÿ
        self._init_mcp_services()
        
        # åˆå§‹åŒ–GRAGè®°å¿†ç³»ç»Ÿï¼ˆåªåœ¨é¦–æ¬¡åˆå§‹åŒ–æ—¶æ˜¾ç¤ºæ—¥å¿—ï¼‰
        self.memory_manager = memory_manager
        if self.memory_manager and not hasattr(self.__class__, '_memory_initialized'):
            logger.info("å¤å›­è®°å¿†ç³»ç»Ÿå·²åˆå§‹åŒ–")
            self.__class__._memory_initialized = True
        
        # åˆå§‹åŒ–è¯­éŸ³å¤„ç†ç³»ç»Ÿ
        self.voice = None
        if config.system.voice_enabled:
            try:
                # è¯­éŸ³åŠŸèƒ½å·²è¿ç§»åˆ°voice_integration.pyï¼Œç”±ui/enhanced_worker.pyè°ƒç”¨
                # ä¸å†éœ€è¦åœ¨è¿™é‡Œåˆå§‹åŒ–VoiceHandler
                # ä½¿ç”¨å…¨å±€å˜é‡é¿å…é‡å¤è¾“å‡ºæ—¥å¿—
                global _VOICE_ENABLED_LOGGED
                if not _VOICE_ENABLED_LOGGED:
                    logger.info("è¯­éŸ³åŠŸèƒ½å·²å¯ç”¨ï¼Œç”±UIå±‚ç®¡ç†")
                    _VOICE_ENABLED_LOGGED = True
            except Exception as e:
                logger.warning(f"è¯­éŸ³ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
                self.voice = None
        
        # æ¢å¤æ ‘çŠ¶æ€è€ƒç³»ç»Ÿ
        self.tree_thinking = None
        # é›†æˆæ ‘çŠ¶æ€è€ƒç³»ç»Ÿï¼ˆå‚è€ƒhandoffçš„å…¨å±€å˜é‡ä¿æŠ¤æœºåˆ¶ï¼‰
        global _TREE_THINKING_SUBSYSTEMS_INITIALIZED
        if not _TREE_THINKING_SUBSYSTEMS_INITIALIZED:
            try:
                self.tree_thinking = TreeThinkingEngine(api_client=self, memory_manager=self.memory_manager)
                print("[TreeThinkingEngine] âœ… æ ‘çŠ¶å¤–ç½®æ€è€ƒç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
                _TREE_THINKING_SUBSYSTEMS_INITIALIZED = True
            except Exception as e:
                logger.warning(f"æ ‘çŠ¶æ€è€ƒç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
                self.tree_thinking = None
        else:
            # å¦‚æœå­ç³»ç»Ÿå·²ç»åˆå§‹åŒ–è¿‡ï¼Œåˆ›å»ºæ–°å®ä¾‹ä½†ä¸é‡æ–°åˆå§‹åŒ–å­ç³»ç»Ÿï¼ˆé™é»˜å¤„ç†ï¼‰
            try:
                self.tree_thinking = TreeThinkingEngine(api_client=self, memory_manager=self.memory_manager)
            except Exception as e:
                logger.warning(f"æ ‘çŠ¶æ€è€ƒç³»ç»Ÿå®ä¾‹åˆ›å»ºå¤±è´¥: {e}")
                self.tree_thinking = None
        
        # åˆå§‹åŒ–å¿«é€Ÿæ¨¡å‹ç®¡ç†å™¨ï¼ˆç”¨äºå¼‚æ­¥æ€è€ƒåˆ¤æ–­ï¼‰
        self.quick_model_manager = None
        # é›†æˆå¿«é€Ÿæ¨¡å‹ç®¡ç†å™¨ï¼ˆå‚è€ƒæ ‘çŠ¶æ€è€ƒçš„å…¨å±€å˜é‡ä¿æŠ¤æœºåˆ¶ï¼‰
        global _QUICK_MODEL_MANAGER_INITIALIZED
        if not _QUICK_MODEL_MANAGER_INITIALIZED:
            try:
                from thinking.quick_model_manager import QuickModelManager
                self.quick_model_manager = QuickModelManager()
                logger.info("å¿«é€Ÿæ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
                _QUICK_MODEL_MANAGER_INITIALIZED = True
            except Exception as e:
                logger.debug(f"å¿«é€Ÿæ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.quick_model_manager = None
        else:
            # å¦‚æœå·²ç»åˆå§‹åŒ–è¿‡ï¼Œåˆ›å»ºæ–°å®ä¾‹ä½†ä¸é‡æ–°åˆå§‹åŒ–ï¼ˆé™é»˜å¤„ç†ï¼‰
            try:
                from thinking.quick_model_manager import QuickModelManager
                self.quick_model_manager = QuickModelManager()
            except Exception as e:
                logger.debug(f"å¿«é€Ÿæ¨¡å‹ç®¡ç†å™¨å®ä¾‹åˆ›å»ºå¤±è´¥: {e}")
                self.quick_model_manager = None
        
        # åˆå§‹åŒ–æƒ…ç»ªAIç³»ç»Ÿ
        self.emotional_ai = None
        if config.emotional_ai.enabled:
            try:
                self.emotional_ai = get_emotion_core(config)
                
                # æ·»åŠ ä¸»åŠ¨å¯¹è¯å›è°ƒ
                def handle_proactive_message(message):
                    """å¤„ç†ä¸»åŠ¨å¯¹è¯æ¶ˆæ¯"""
                    try:
                        # è¿™é‡Œå¯ä»¥å‘é€åˆ°UIæˆ–è®°å½•æ—¥å¿—
                        emotion_display = self.emotional_ai.get_emotion_display()
                        logger.info(f"[{config.emotional_ai.ai_name}] {message} [{emotion_display}]")
                        
                        # å¦‚æœæœ‰UIå›è°ƒï¼Œå¯ä»¥åœ¨è¿™é‡Œè°ƒç”¨
                        if hasattr(self, '_ui_callback') and self._ui_callback:
                            self._ui_callback(f"ğŸ¤– {config.emotional_ai.ai_name}: {message}")
                    except Exception as e:
                        logger.error(f"å¤„ç†ä¸»åŠ¨æ¶ˆæ¯å¤±è´¥: {e}")
                
                self.emotional_ai.add_proactive_callback(handle_proactive_message)
                logger.info(f"æƒ…ç»ªAIç³»ç»Ÿå·²åˆå§‹åŒ– - {config.emotional_ai.ai_name} ({config.emotional_ai.personality_age}å²)")
            except Exception as e:
                logger.warning(f"æƒ…ç»ªAIç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
                self.emotional_ai = None
    
    def _builtin_demo_response(self, user_input: str, emotion_state=None) -> str:
        """å†…ç½®æ¼”ç¤ºå“åº”"""
        import random
        
        responses = {
            "ä½ å¥½": ["ä½ å¥½å‘€ï¼æˆ‘æ˜¯StarryNightï¼ğŸ˜Š", "å—¨ï½å¾ˆé«˜å…´è§åˆ°ä½ ï¼"],
            "å†è§": ["å†è§ï½è®°å¾—å¸¸æ¥é™ªæˆ‘å“¦ï¼", "æ‹œæ‹œï¼æˆ‘ä¼šæƒ³ä½ çš„ï¼"],
            "ä½ çœŸæ£’": ["è°¢è°¢å¤¸å¥–ï¼æˆ‘å¥½å¼€å¿ƒï¼ğŸ˜Š", "å˜»å˜»ï¼Œä½ ä¹Ÿå¾ˆæ£’å‘€ï¼"],
            "ä¸ºä»€ä¹ˆ": ["è¿™æ˜¯ä¸ªå¥½é—®é¢˜ï¼ğŸ¤”", "è®©æˆ‘æƒ³æƒ³...ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ"],
            "æ¸¸æˆ": ["æˆ‘ä»¬ç©ä»€ä¹ˆæ¸¸æˆå‘¢ï¼ŸğŸ¤©", "æ¸¸æˆï¼æˆ‘æœ€å–œæ¬¢äº†ï¼"],
            "test": ["æ¼”ç¤ºæ¨¡å¼è¿è¡Œæ­£å¸¸ï¼âœ…", "æµ‹è¯•æˆåŠŸï½æˆ‘åœ¨è¿™é‡Œå‘¢ï¼"]
        }
        
        # å…³é”®è¯åŒ¹é…
        for keyword, resp_list in responses.items():
            if keyword in user_input:
                return random.choice(resp_list)
        
        # é»˜è®¤å“åº”
        default_responses = [
            "æˆ‘åœ¨è¿™é‡Œå‘¢ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ",
            "è®©æˆ‘æƒ³æƒ³æ€ä¹ˆå›ç­”...",
            "å—¯å—¯ï¼Œæˆ‘å¬ç€å‘¢ï½",
            "è¿™ä¸ªè¯é¢˜çœŸæœ‰è¶£ï¼",
            "æˆ‘æ­£åœ¨å­¦ä¹ ä¸­...âœ¨"
        ]
        
        return random.choice(default_responses)

    def set_ui_callback(self, callback):
        """è®¾ç½®UIå›è°ƒå‡½æ•°"""
        self._ui_callback = callback
    
    def get_emotional_status(self):
        """è·å–æƒ…ç»ªçŠ¶æ€"""
        if self.emotional_ai:
            return self.emotional_ai.get_emotion_status()
        return None
    
    def _init_mcp_services(self):
        """åˆå§‹åŒ–MCPæœåŠ¡ç³»ç»Ÿï¼ˆåªåœ¨é¦–æ¬¡åˆå§‹åŒ–æ—¶è¾“å‡ºæ—¥å¿—ï¼Œåç»­é™é»˜ï¼‰"""
        global _MCP_SERVICES_INITIALIZED
        if _MCP_SERVICES_INITIALIZED:
            # é™é»˜è·³è¿‡ï¼Œä¸è¾“å‡ºä»»ä½•æ—¥å¿—
            return
        try:
            # è‡ªåŠ¨æ³¨å†Œæ‰€æœ‰MCPæœåŠ¡å’Œhandoff
            self.mcp.auto_register_services()
            logger.info("MCPæœåŠ¡ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            _MCP_SERVICES_INITIALIZED = True
        except Exception as e:
            logger.error(f"MCPæœåŠ¡ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")

    def save_log(self, u, a):  # ä¿å­˜å¯¹è¯æ—¥å¿—
        if self.dev_mode:
            return  # å¼€å‘è€…æ¨¡å¼ä¸å†™æ—¥å¿—
        d = datetime.now().strftime('%Y-%m-%d')
        t = datetime.now().strftime('%H:%M:%S')
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        log_dir = config.system.log_dir
        if not os.path.exists(log_dir):
            os.makedirs(log_dir, exist_ok=True)
            logger.info(f"å·²åˆ›å»ºæ—¥å¿—ç›®å½•: {log_dir}")
        
        f = os.path.join(log_dir, f'{d}.txt')
        with open(f, 'a', encoding='utf-8') as w:
            w.write('-'*50 + f'\næ—¶é—´: {d} {t}\nç”¨æˆ·: {u}\nStarryNight: {a}\n\n')

    async def _call_llm(self, messages: List[Dict]) -> Dict:
        """è°ƒç”¨LLM API"""
        try:
            resp = await self.async_client.chat.completions.create(
                model=config.api.model, 
                messages=messages, 
                temperature=config.api.temperature, 
                max_tokens=config.api.max_tokens, 
                stream=False  # å·¥å…·è°ƒç”¨å¾ªç¯ä¸­ä¸ä½¿ç”¨æµå¼
            )
            return {
                'content': resp.choices[0].message.content,
                'status': 'success'
            }
        except RuntimeError as e:
            if "handler is closed" in str(e):
                logger.debug(f"å¿½ç•¥è¿æ¥å…³é—­å¼‚å¸¸: {e}")
                # é‡æ–°åˆ›å»ºå®¢æˆ·ç«¯å¹¶é‡è¯•
                self.async_client = AsyncOpenAI(api_key=config.api.api_key, base_url=config.api.base_url.rstrip('/') + '/')
                resp = await self.async_client.chat.completions.create(
                    model=config.api.model, 
                    messages=messages, 
                    temperature=config.api.temperature, 
                    max_tokens=config.api.max_tokens, 
                    stream=False
                )
                return {
                    'content': resp.choices[0].message.content,
                    'status': 'success'
                }
            else:
                raise
        except Exception as e:
            logger.error(f"LLM APIè°ƒç”¨å¤±è´¥: {e}")
            return {
                'content': f"APIè°ƒç”¨å¤±è´¥: {str(e)}",
                'status': 'error'
            }

    # å·¥å…·è°ƒç”¨å¾ªç¯ç›¸å…³æ–¹æ³•
    def handle_llm_response(self, a, mcp):
        # åªä¿ç•™æ™®é€šæ–‡æœ¬æµå¼è¾“å‡ºé€»è¾‘ #
        async def text_stream():
            for line in a.splitlines():
                yield ("StarryNight", line)
        return text_stream()

    def _format_services_for_prompt(self, available_services: dict) -> str:
        """æ ¼å¼åŒ–å¯ç”¨æœåŠ¡åˆ—è¡¨ä¸ºpromptå­—ç¬¦ä¸²ï¼ŒMCPæœåŠ¡å’ŒAgentæœåŠ¡åˆ†å¼€ï¼ŒåŒ…å«å…·ä½“è°ƒç”¨æ ¼å¼"""
        mcp_services = available_services.get("mcp_services", [])
        agent_services = available_services.get("agent_services", [])
        
        # è·å–æœ¬åœ°åŸå¸‚ä¿¡æ¯å’Œå½“å‰æ—¶é—´
        local_city = "æœªçŸ¥åŸå¸‚"
        current_time = ""
        try:
            # ä»WeatherTimeAgentè·å–æœ¬åœ°åŸå¸‚ä¿¡æ¯
            from mcpserver.agent_weather_time.agent_weather_time import WeatherTimeTool
            weather_tool = WeatherTimeTool()
            local_city = getattr(weather_tool, '_local_city', 'æœªçŸ¥åŸå¸‚') or 'æœªçŸ¥åŸå¸‚'
            
            # è·å–å½“å‰æ—¶é—´
            from datetime import datetime
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        except Exception as e:
            print(f"[DEBUG] è·å–æœ¬åœ°ä¿¡æ¯å¤±è´¥: {e}")
        
        # æ ¼å¼åŒ–MCPæœåŠ¡åˆ—è¡¨ï¼ŒåŒ…å«å…·ä½“è°ƒç”¨æ ¼å¼
        mcp_list = []
        for service in mcp_services:
            name = service.get("name", "")
            description = service.get("description", "")
            display_name = service.get("display_name", name)
            tools = service.get("available_tools", [])
            
            # å±•ç¤ºname+displayName
            if description:
                mcp_list.append(f"- {name}: {description}")
            else:
                mcp_list.append(f"- {name}")
            
            # ä¸ºæ¯ä¸ªå·¥å…·æ˜¾ç¤ºå…·ä½“è°ƒç”¨æ ¼å¼
            if tools:
                for tool in tools:
                    tool_name = tool.get('name', '')
                    tool_desc = tool.get('description', '')
                    tool_example = tool.get('example', '')
                    
                    if tool_name and tool_example:
                        # è§£æç¤ºä¾‹JSONï¼Œæå–å‚æ•°
                        try:
                            import json
                            example_data = json.loads(tool_example)
                            params = []
                            for key, value in example_data.items():
                                if key != 'tool_name':
                                    # ç‰¹æ®Šå¤„ç†cityå‚æ•°ï¼Œæ³¨å…¥æœ¬åœ°åŸå¸‚ä¿¡æ¯
                                    if key == 'city' and name == 'WeatherTimeAgent':
                                        params.append(f"{key}: {local_city}")
                                    else:
                                        params.append(f"{key}: {value}")
                            
                            # æ„å»ºè°ƒç”¨æ ¼å¼
                            format_str = f"  {tool_name}: ï½›\n"
                            format_str += f"    \"agentType\": \"mcp\",\n"
                            format_str += f"    \"service_name\": \"{name}\",\n"
                            format_str += f"    \"tool_name\": \"{tool_name}\",\n"
                            for param in params:
                                # å°†ä¸­æ–‡å‚æ•°åè½¬æ¢ä¸ºè‹±æ–‡
                                param_key, param_value = param.split(': ', 1)
                                if param_key == 'city' and name == 'WeatherTimeAgent':
                                    format_str += f"    \"{param_key}\": \"{local_city}\",\n"
                                else:
                                    format_str += f"    \"{param_key}\": \"{param_value}\",\n"
                            format_str += f"  ï½\n"
                            
                            mcp_list.append(format_str)
                        except:
                            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ ¼å¼
                            mcp_list.append(f"  {tool_name}: ä½¿ç”¨tool_nameå‚æ•°è°ƒç”¨")
        
        # æ ¼å¼åŒ–AgentæœåŠ¡åˆ—è¡¨
        agent_list = []
        
        # 1. æ·»åŠ handoffæœåŠ¡
        for service in agent_services:
            name = service.get("name", "")
            description = service.get("description", "")
            tool_name = service.get("tool_name", "agent")
            display_name = service.get("display_name", name)
            # å±•ç¤ºname+displayName
            if description:
                agent_list.append(f"- {name}(å·¥å…·å: {tool_name}): {description}")
            else:
                agent_list.append(f"- {name}(å·¥å…·å: {tool_name})")
        
        # 2. ç›´æ¥ä»AgentManagerè·å–å·²æ³¨å†Œçš„Agent
        try:
            from mcpserver.agent_manager import get_agent_manager
            agent_manager = get_agent_manager()
            agent_manager_agents = agent_manager.get_available_agents()
            
            for agent in agent_manager_agents:
                name = agent.get("name", "")
                base_name = agent.get("base_name", "")
                description = agent.get("description", "")
                
                # å±•ç¤ºæ ¼å¼ï¼šbase_name: æè¿°
                if description:
                    agent_list.append(f"- {base_name}: {description}")
                else:
                    agent_list.append(f"- {base_name}")
                    
        except Exception as e:
            # å¦‚æœAgentManagerä¸å¯ç”¨ï¼Œé™é»˜å¤„ç†
            pass
        
        # æ·»åŠ æœ¬åœ°ä¿¡æ¯è¯´æ˜
        local_info = f"\n\nã€å½“å‰ç¯å¢ƒä¿¡æ¯ã€‘\n- æœ¬åœ°åŸå¸‚: {local_city}\n- å½“å‰æ—¶é—´: {current_time}\n\nã€ä½¿ç”¨è¯´æ˜ã€‘\n- å¤©æ°”/æ—¶é—´æŸ¥è¯¢æ—¶ï¼Œè¯·ä½¿ç”¨ä¸Šè¿°æœ¬åœ°åŸå¸‚ä¿¡æ¯ä½œä¸ºcityå‚æ•°\n- æ‰€æœ‰æ—¶é—´ç›¸å…³æŸ¥è¯¢éƒ½åŸºäºå½“å‰ç³»ç»Ÿæ—¶é—´"
        
        # è¿”å›æ ¼å¼åŒ–çš„æœåŠ¡åˆ—è¡¨
        result = {
            "available_mcp_services": "\n".join(mcp_list) + local_info if mcp_list else "æ— " + local_info,
            "available_agent_services": "\n".join(agent_list) if agent_list else "æ— "
        }
        
        return result

    async def process(self, u, is_voice_input=False):  # æ·»åŠ is_voice_inputå‚æ•°
        try:
            # æ¼”ç¤ºæ¨¡å¼å¤„ç†
            if self.demo_mode:
                # è·å–å½“å‰æƒ…ç»ªçŠ¶æ€
                emotion_state = None
                if self.emotional_ai:
                    try:
                        emotions = self.emotional_ai.current_emotions
                        if emotions:
                            emotion_state = emotions[0].emotion.value
                    except:
                        pass
                
                # ç”Ÿæˆæ¼”ç¤ºå“åº”
                response = self.demo_response_func(u, emotion_state)
                
                # æƒ…ç»ªAIå¤„ç†
                if self.emotional_ai:
                    try:
                        self.emotional_ai.process_interaction(u, response)
                    except Exception as e:
                        logger.error(f"æ¼”ç¤ºæ¨¡å¼æƒ…ç»ªAIå¤„ç†å¤±è´¥: {e}")
                
                yield ("StarryNight", response)
                return
            
            # è‡ªç„¶è¯­è¨€å¤„ç† - æ£€æµ‹å¹¶æ‰§è¡Œé«˜çº§åŠŸèƒ½
            enhanced_context = ""
            try:
                from natural_language_processor import natural_language_processor
                
                nlp_result = await natural_language_processor.process_user_input(u)
                if nlp_result['detected_functions']:
                    logger.info(f"æ£€æµ‹åˆ°é«˜çº§åŠŸèƒ½è°ƒç”¨: {nlp_result['detected_functions']}")
                    enhanced_context = nlp_result['enhanced_context']
                    
                    # å¦‚æœæ£€æµ‹åˆ°åŠŸèƒ½è°ƒç”¨ï¼Œä½¿ç”¨å¢å¼ºçš„ä¸Šä¸‹æ–‡
                    if enhanced_context:
                        u = enhanced_context
                        
            except Exception as e:
                logger.debug(f"è‡ªç„¶è¯­è¨€å¤„ç†å¤±è´¥: {e}")
            
            # å¼€å‘è€…æ¨¡å¼ä¼˜å…ˆåˆ¤æ–­
            if u.strip() == "#devmode":
                self.dev_mode = True
                yield ("StarryNight", "å·²è¿›å…¥å¼€å‘è€…æ¨¡å¼")
                return

            # åªåœ¨è¯­éŸ³è¾“å…¥æ—¶æ˜¾ç¤ºå¤„ç†æç¤º
            if is_voice_input:
                print(f"å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥ï¼š{now()}")  # è¯­éŸ³è½¬æ–‡æœ¬ç»“æŸï¼Œå¼€å§‹å¤„ç†
            
            # å®Œå…¨ç¦ç”¨GRAGè®°å¿†æŸ¥è¯¢
            # GRAGè®°å¿†æŸ¥è¯¢
            # memory_context = ""
            # if self.memory_manager:
            #     try:
            #         memory_result = await self.memory_manager.query_memory(u)
            #         if memory_result:
            #             # memory_context = f"\n[è®°å¿†æ£€ç´¢ç»“æœ]: {memory_result}\n"
            #             logger.info("ä»GRAGè®°å¿†ä¸­æ£€ç´¢åˆ°ç›¸å…³ä¿¡æ¯")
            #     except Exception as e:
            #         logger.error(f"GRAGè®°å¿†æŸ¥è¯¢å¤±è´¥: {e}")
            
            # è·å–äººè®¾æç¤ºè¯
            persona_prompt = ""
            try:
                from persona_management_system import get_persona_prompt, get_persona_manager, record_ai_behavior
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°äººè®¾
                persona_manager = get_persona_manager()
                if hasattr(persona_manager, 'should_update_llm_persona') and persona_manager.should_update_llm_persona():
                    persona_prompt = get_persona_prompt(f"ç”¨æˆ·è¯´: {u}")
                    
                    # è®°å½•äººè®¾æ›´æ–°è¡Œä¸º
                    record_ai_behavior(
                        "persona_update",
                        "æ›´æ–°LLMäººè®¾ä¿¡æ¯ä»¥é€‚åº”å½“å‰æƒ…ç»ªå’ŒçŠ¶æ€",
                        emotional_impact=0.2
                    )
                    logger.info("å·²æ›´æ–°LLMäººè®¾æç¤ºè¯")
                else:
                    # ä½¿ç”¨ç®€åŒ–äººè®¾ï¼Œé¿å…è¿‡åº¦å†—ä½™
                    persona_prompt = "You are StarryNight, a cute AI assistant with the mental age of 3. You'll stay lively and adorable!ï¼ŒRespond naturally based on the current mood."
                
            except Exception as e:
                logger.error(f"è·å–äººè®¾æç¤ºè¯å¤±è´¥: {e}")
                persona_prompt = "You are StarryNight, a cute AI assistant with the mental age of 3. You'll stay lively and adorable!"
            
            # ç»“åˆäººè®¾å’ŒåŸæœ‰ç³»ç»Ÿæç¤ºè¯
            enhanced_system_prompt = f"{persona_prompt}\n\n{RECOMMENDED_PROMPT_PREFIX}\n{config.prompts.naga_system_prompt}"
            
            # è·å–è¿‡æ»¤åçš„æœåŠ¡åˆ—è¡¨
            available_services = self.mcp.get_available_services_filtered()
            services_text = self._format_services_for_prompt(available_services)
            
            # å®‰å…¨åœ°æ ¼å¼åŒ–ç³»ç»Ÿæç¤ºè¯
            try:
                formatted_prompt = enhanced_system_prompt.format(**services_text)
            except KeyError as e:
                logger.warning(f"ç³»ç»Ÿæç¤ºè¯æ ¼å¼åŒ–å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤æç¤ºè¯")
                formatted_prompt = enhanced_system_prompt.replace("{available_mcp_services}", services_text.get("available_mcp_services", "æ— ")).replace("{available_agent_services}", services_text.get("available_agent_services", "æ— "))
            except Exception as e:
                logger.error(f"ç³»ç»Ÿæç¤ºè¯æ ¼å¼åŒ–å¼‚å¸¸: {e}, ä½¿ç”¨åŸºç¡€æç¤ºè¯")
                formatted_prompt = "You are StarryNight, a helpful AI assistant."
            
            sysmsg = {"role": "system", "content": formatted_prompt}
            msgs = [sysmsg] if sysmsg else []
            msgs += self.messages[-20:] + [{"role": "user", "content": u}]

            print(f"GTPè¯·æ±‚å‘é€ï¼š{now()}")  # AIè¯·æ±‚å‰
            
            # éçº¿æ€§æ€è€ƒåˆ¤æ–­ï¼šå¯åŠ¨åå°å¼‚æ­¥åˆ¤æ–­ä»»åŠ¡
            thinking_task = None
            if hasattr(self, 'tree_thinking') and self.tree_thinking and getattr(self.tree_thinking, 'is_enabled', False):
                # å¯åŠ¨å¼‚æ­¥æ€è€ƒåˆ¤æ–­ä»»åŠ¡
                import asyncio
                thinking_task = asyncio.create_task(self._async_thinking_judgment(u))
            
            # æ™®é€šæ¨¡å¼ï¼šèµ°å·¥å…·è°ƒç”¨å¾ªç¯ï¼ˆä¸ç­‰å¾…æ€è€ƒæ ‘åˆ¤æ–­ï¼‰
            try:
                result = await tool_call_loop(msgs, self.mcp, self._call_llm, is_streaming=True)
                final_content = result['content']
                recursion_depth = result['recursion_depth']
                
                if recursion_depth > 0:
                    print(f"å·¥å…·è°ƒç”¨å¾ªç¯å®Œæˆï¼Œå…±æ‰§è¡Œ {recursion_depth} è½®")
                
                # æµå¼è¾“å‡ºæœ€ç»ˆç»“æœ
                for line in final_content.splitlines():
                    yield ("StarryNight", line)
                
                # æƒ…ç»ªAIå¤„ç†
                if self.emotional_ai:
                    try:
                        # å¤„ç†ç”¨æˆ·äº¤äº’ï¼Œæ›´æ–°æƒ…ç»ªçŠ¶æ€
                        self.emotional_ai.process_interaction(u, final_content)
                        
                        # æ ¹æ®æƒ…ç»ªä¿®æ”¹å›å¤ï¼ˆå¯é€‰ï¼Œä¿æŒåŸæœ‰å›å¤çš„å®Œæ•´æ€§ï¼‰
                        # final_content = self.emotional_ai.get_personality_modifier(final_content)
                    except Exception as e:
                        logger.error(f"æƒ…ç»ªAIå¤„ç†å¤±è´¥: {e}")
                
                # ä¿å­˜å¯¹è¯å†å²
                self.messages += [{"role": "user", "content": u}, {"role": "assistant", "content": final_content}]
                self.save_log(u, final_content)
                
                # GRAGè®°å¿†å­˜å‚¨ï¼ˆå¼€å‘è€…æ¨¡å¼ä¸å†™å…¥ï¼‰
                if self.memory_manager and not self.dev_mode:
                    try:
                        await self.memory_manager.add_conversation_memory(u, final_content)
                    except Exception as e:
                        logger.error(f"GRAGè®°å¿†å­˜å‚¨å¤±è´¥: {e}")
                
                # åŠ¨æ€å‘å¸ƒå™¨ - å‘å¸ƒå¯¹è¯åŠ¨æ€
                if DYNAMIC_PUBLISHER_AVAILABLE and not self.dev_mode:
                    try:
                        # æ„å»ºå¯¹è¯æ‘˜è¦
                        conversation_summary = f"ç”¨æˆ·ï¼š{u[:50]}{'...' if len(u) > 50 else ''}\nStarryNightï¼š{final_content[:100]}{'...' if len(final_content) > 100 else ''}"
                        
                        # å¼‚æ­¥å‘å¸ƒå¯¹è¯åŠ¨æ€
                        await ai_dynamic_publisher.queue_activity(
                            'conversation',
                            conversation_summary,
                            {
                                'user_input': u,
                                'ai_response': final_content,
                                'recursion_depth': recursion_depth,
                                'timestamp': datetime.now().isoformat(),
                                'input_method': 'voice' if is_voice_input else 'text'
                            }
                        )
                        logger.debug("å¯¹è¯åŠ¨æ€å·²åŠ å…¥å‘å¸ƒé˜Ÿåˆ—")
                    except Exception as e:
                        logger.error(f"å¯¹è¯åŠ¨æ€å‘å¸ƒå¤±è´¥: {e}")
                
                # æ£€æŸ¥å¼‚æ­¥æ€è€ƒåˆ¤æ–­ç»“æœï¼Œå¦‚æœå»ºè®®æ·±åº¦æ€è€ƒåˆ™æç¤ºç”¨æˆ·
                if thinking_task and not thinking_task.done():
                    # ç­‰å¾…æ€è€ƒåˆ¤æ–­å®Œæˆï¼ˆæœ€å¤šç­‰å¾…3ç§’ï¼‰
                    try:
                        await asyncio.wait_for(thinking_task, timeout=3.0)
                        if thinking_task.result():
                            yield ("StarryNight", "\nğŸ’¡ è¿™ä¸ªé—®é¢˜è¾ƒä¸ºå¤æ‚ï¼Œä¸‹é¢æˆ‘ä¼šæ›´è¯¦ç»†åœ°è§£é‡Šè¿™ä¸ªæµç¨‹...")
                            # å¯åŠ¨æ·±åº¦æ€è€ƒ
                            try:
                                thinking_result = await self.tree_thinking.think_deeply(u)
                                if thinking_result and "answer" in thinking_result:
                                    # ç›´æ¥ä½¿ç”¨thinkingç³»ç»Ÿçš„ç»“æœï¼Œé¿å…é‡å¤å¤„ç†
                                    yield ("StarryNight", f"\n{thinking_result['answer']}")
                                    
                                    # æ›´æ–°å¯¹è¯å†å²
                                    final_thinking_answer = thinking_result['answer']
                                    self.messages[-1] = {"role": "assistant", "content": final_content + "\n\n" + final_thinking_answer}
                                    self.save_log(u, final_content + "\n\n" + final_thinking_answer)
                                    
                                    # GRAGè®°å¿†å­˜å‚¨ï¼ˆå¼€å‘è€…æ¨¡å¼ä¸å†™å…¥ï¼‰
                                    if self.memory_manager and not self.dev_mode:
                                        try:
                                            await self.memory_manager.add_conversation_memory(u, final_content + "\n\n" + final_thinking_answer)
                                        except Exception as e:
                                            logger.error(f"GRAGè®°å¿†å­˜å‚¨å¤±è´¥: {e}")
                            except Exception as e:
                                logger.error(f"æ·±åº¦æ€è€ƒå¤„ç†å¤±è´¥: {e}")
                                yield ("StarryNight", f"ğŸŒ³ æ·±åº¦æ€è€ƒç³»ç»Ÿå‡ºé”™: {str(e)}")
                    except asyncio.TimeoutError:
                        # è¶…æ—¶å–æ¶ˆä»»åŠ¡
                        thinking_task.cancel()
                    except Exception as e:
                        logger.debug(f"æ€è€ƒåˆ¤æ–­ä»»åŠ¡å¼‚å¸¸: {e}")
                
            except Exception as e:
                print(f"å·¥å…·è°ƒç”¨å¾ªç¯å¤±è´¥: {e}")
                yield ("StarryNight", f"[MCPå¼‚å¸¸]: {e}")
                return

            return
        except Exception as e:
            import sys
            import traceback
            traceback.print_exc(file=sys.stderr)
            yield ("StarryNight", f"[MCPå¼‚å¸¸]: {e}")
            return

    async def get_response(self, prompt: str, temperature: float = 0.7) -> str:
        """ä¸ºæ ‘çŠ¶æ€è€ƒç³»ç»Ÿç­‰æä¾›APIè°ƒç”¨æ¥å£""" # ç»Ÿä¸€æ¥å£
        try:
            response = await self.async_client.chat.completions.create(
                model=config.api.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=config.api.max_tokens
            )
            return response.choices[0].message.content
        except RuntimeError as e:
            if "handler is closed" in str(e):
                logger.debug(f"å¿½ç•¥è¿æ¥å…³é—­å¼‚å¸¸ï¼Œé‡æ–°åˆ›å»ºå®¢æˆ·ç«¯: {e}")
                # é‡æ–°åˆ›å»ºå®¢æˆ·ç«¯å¹¶é‡è¯•
                self.async_client = AsyncOpenAI(api_key=config.api.api_key, base_url=config.api.base_url.rstrip('/') + '/')
                response = await self.async_client.chat.completions.create(
                    model=config.api.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=temperature,
                    max_tokens=config.api.max_tokens
                )
                return response.choices[0].message.content
            else:
                logger.error(f"APIè°ƒç”¨å¤±è´¥: {e}")
                return f"APIè°ƒç”¨å‡ºé”™: {str(e)}"
        except Exception as e:
            logger.error(f"APIè°ƒç”¨å¤±è´¥: {e}")
            return f"APIè°ƒç”¨å‡ºé”™: {str(e)}"

    async def _async_thinking_judgment(self, question: str) -> bool:
        """å¼‚æ­¥åˆ¤æ–­é—®é¢˜æ˜¯å¦éœ€è¦æ·±åº¦æ€è€ƒ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            bool: æ˜¯å¦éœ€è¦æ·±åº¦æ€è€ƒ
        """
        try:
            if not self.tree_thinking:
                return False
            
            # ä½¿ç”¨thinkingæ–‡ä»¶å¤¹ä¸­ç°æˆçš„éš¾åº¦åˆ¤æ–­å™¨
            difficulty_assessment = await self.tree_thinking.difficulty_judge.assess_difficulty(question)
            difficulty = difficulty_assessment.get("difficulty", 3)
            
            # æ ¹æ®éš¾åº¦åˆ¤æ–­æ˜¯å¦éœ€è¦æ·±åº¦æ€è€ƒ
            # éš¾åº¦4-5ï¼ˆå¤æ‚/æéš¾ï¼‰å»ºè®®æ·±åº¦æ€è€ƒ
            should_think_deeply = difficulty >= 4
            
            logger.info(f"éš¾åº¦åˆ¤æ–­ï¼š{difficulty}/5ï¼Œå»ºè®®æ·±åº¦æ€è€ƒï¼š{should_think_deeply}")
            return should_think_deeply
                   
        except Exception as e:
            logger.debug(f"å¼‚æ­¥æ€è€ƒåˆ¤æ–­å¤±è´¥: {e}")
            return False

async def process_user_message(s,msg):
    if config.system.voice_enabled and not msg: #æ— æ–‡æœ¬è¾“å…¥æ—¶å¯åŠ¨è¯­éŸ³è¯†åˆ«
        async for text in s.voice.stt_stream():
            if text:
                msg=text
                break
        return await s.process(msg, is_voice_input=True)  # è¯­éŸ³è¾“å…¥
    return await s.process(msg, is_voice_input=False)  # æ–‡å­—è¾“å…¥

# å…¨å±€LLM APIè°ƒç”¨å‡½æ•°ï¼Œä¾›å…¶ä»–æ¨¡å—ä½¿ç”¨
async def call_llm_api(prompt: str, max_tokens: int = 500, temperature: float = 0.7) -> str:
    """
    å…¨å±€LLM APIè°ƒç”¨å‡½æ•°
    
    Args:
        prompt: æç¤ºè¯
        max_tokens: æœ€å¤§tokenæ•°
        temperature: æ¸©åº¦å‚æ•°
        
    Returns:
        str: LLMå“åº”æ–‡æœ¬
    """
    client = None
    try:
        from openai import AsyncOpenAI
        
        client = AsyncOpenAI(
            api_key=config.api.api_key,
            base_url=config.api.base_url.rstrip('/') + '/'
        )
        
        response = await client.chat.completions.create(
            model=config.api.model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        result = response.choices[0].message.content.strip()
        return result
        
    except Exception as e:
        logger.error(f"LLM APIè°ƒç”¨å¤±è´¥: {e}")
        return f"æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•æ€è€ƒè¿™ä¸ªé—®é¢˜ï¼š{e}"
    finally:
        # ç¡®ä¿å®¢æˆ·ç«¯æ­£ç¡®å…³é—­
        if client is not None:
            try:
                await client.close()
            except Exception as e:
                logger.debug(f"å…³é—­LLMå®¢æˆ·ç«¯æ—¶å‡ºç°å¼‚å¸¸: {e}")
                pass